"""
üìö InfoAgent v4 ‚Äî factual y sin invenciones
Responde preguntas generales sobre el hotel.
Usa la base de conocimiento (MCP) y escala al encargado si no hay informaci√≥n v√°lida.
"""

import re
import logging
import asyncio

# Core imports
from core.language_manager import language_manager
from core.utils.normalize_reply import normalize_reply
from core.mcp_client import mcp_client
from core.utils.time_context import get_time_context
from core.utils.utils_prompt import load_prompt
from core.config import ModelConfig, ModelTier  # ‚úÖ configuraci√≥n centralizada

log = logging.getLogger("InfoAgent")


# =============================================================
# üîç Detecci√≥n de dumps t√©cnicos o respuestas an√≥malas
# =============================================================
def _looks_like_internal_dump(text: str) -> bool:
    """Detecta texto t√©cnico interno o volcado an√≥malo."""
    if not text:
        return False

    dump_patterns = ["traceback", "error", "exception", "{", "}", "SELECT ", "sql", "schema"]
    if any(pat.lower() in text.lower() for pat in dump_patterns):
        return True

    keywords_ok = [
        "gimnasio", "desayuno", "recepci√≥n", "parking",
        "mascotas", "wifi", "check-in", "restaurante",
        "habitaciones", "coworking", "lavander√≠a"
    ]
    if len(text.split()) > 1200 and not any(k in text.lower() for k in keywords_ok):
        return True

    return False


# =============================================================
# üß© Tool principal ‚Äî consulta factual a MCP
# =============================================================
async def hotel_information_tool(query: str) -> str:
    """Consulta factual desde la base de conocimiento (MCP)."""
    try:
        q = (query or "").strip()
        if not q:
            return "ESCALATION_REQUIRED"

        tools = await mcp_client.get_tools(server_name="InfoAgent")
        if not tools:
            log.warning("‚ö†Ô∏è No se encontraron herramientas MCP para InfoAgent.")
            return "ESCALATION_REQUIRED"

        info_tool = next((t for t in tools if "conocimiento" in t.name.lower()), None)
        if not info_tool:
            log.warning("‚ö†Ô∏è No se encontr√≥ 'Base_de_conocimientos_del_hotel' en MCP.")
            return "ESCALATION_REQUIRED"

        raw_reply = await info_tool.ainvoke({"input": q})
        cleaned = normalize_reply(raw_reply, q, "InfoAgent").strip()

        if not cleaned or len(cleaned) < 10:
            return "ESCALATION_REQUIRED"
        if _looks_like_internal_dump(cleaned):
            return "ESCALATION_REQUIRED"
        if "no hay resultados" in cleaned.lower() or "no encontrado" in cleaned.lower():
            return "ESCALATION_REQUIRED"

        cleaned = re.sub(r"[*#>\-]+", "", cleaned)
        cleaned = re.sub(r"\s{2,}", " ", cleaned).strip()
        return cleaned

    except Exception as e:
        log.error(f"‚ùå Error en hotel_information_tool: {e}", exc_info=True)
        return "ESCALATION_REQUIRED"


# =============================================================
# üß† InfoAgent ‚Äî factual, solicita confirmaci√≥n de escalaci√≥n
# =============================================================
class InfoAgent:
    """Agente factual ‚Äî usa ModelConfig y prompt de utils_prompt."""

    def __init__(self, memory_manager=None, model_name=None, temperature=None):
        """
        Args:
            memory_manager: Gestor de memoria contextual.
            model_name: (opcional) Modelo a usar. Si no se pasa, se toma del ModelConfig centralizado.
            temperature: (opcional) Temperatura del modelo.
        """
        self.memory_manager = memory_manager

        if model_name or temperature:
            from langchain_openai import ChatOpenAI
            name = model_name or "gpt-4.1"
            temp = temperature if temperature is not None else 0.3
            self.llm = ChatOpenAI(model=name, temperature=temp)
        else:
            self.llm = ModelConfig.get_llm(ModelTier.SUBAGENT)

        base_prompt = load_prompt("info_hotel_prompt.txt") or (
            "Eres un agente de informaci√≥n del hotel. "
            "Responde solo con datos verificables de la base MCP y escala al encargado si no tienes informaci√≥n."
        )
        self.prompt_text = f"{get_time_context()}\n\n{base_prompt.strip()}"

        log.info(f"‚úÖ InfoAgent inicializado (modelo={self.llm.model_name})")

    # --------------------------------------------------
    def _sync_run(self, coro, *args, **kwargs):
        """Ejecuta async dentro de sync context (para compatibilidad LangChain)."""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()
        return loop.run_until_complete(coro(*args, **kwargs))

    # --------------------------------------------------
    async def invoke(self, user_input: str, chat_history: list = None, chat_id: str = None) -> str:
        """Responde consultas factuales del hu√©sped."""
        log.info(f"üì© [InfoAgent] Consulta: {user_input}")
        lang = language_manager.detect_language(user_input)
        chat_history = chat_history or []

        try:
            respuesta_final = await hotel_information_tool(user_input)

            if respuesta_final == "ESCALATION_REQUIRED":
                log.warning("‚ö†Ô∏è La base MCP no devolvi√≥ informaci√≥n suficiente. Se solicitar√° confirmaci√≥n al hu√©sped.")
                if self.memory_manager and chat_id:
                    self.memory_manager.update_memory(
                        chat_id,
                        role="system",
                        content=(
                            "[InfoAgent] Base de conocimiento sin datos √∫tiles. "
                            "Se recomienda confirmar escalaci√≥n con el encargado."
                        ),
                    )
                return "ESCALATION_REQUIRED"

            respuesta_final = language_manager.ensure_language(respuesta_final, lang)

            if self.memory_manager and chat_id:
                self.memory_manager.update_memory(
                    chat_id,
                    role="assistant",
                    content=f"[InfoAgent] Entrada: {user_input}\n\nRespuesta factual: {respuesta_final}"
                )

            lower_response = respuesta_final.lower()
            no_info = any(
                p in lower_response
                for p in [
                    "no dispongo",
                    "no disponemos",
                    "no tengo informaci√≥n",
                    "consultarlo con el encargado",
                    "perm√≠teme contactar",
                    "no hay resultados",
                    "no encontrado",
                    "¬øte gustar√≠a consultar por alg√∫n otro servicio",
                    "te gustar√≠a consultar por alg√∫n otro servicio",
                    "te gustaria consultar por algun otro servicio",
                ]
            )

            if _looks_like_internal_dump(respuesta_final) or no_info:
                log.warning("‚ö†Ô∏è Respuesta ambigua o insuficiente. Se solicitar√° confirmaci√≥n de escalaci√≥n.")
                if self.memory_manager and chat_id:
                    self.memory_manager.update_memory(
                        chat_id,
                        role="system",
                        content=(
                            "[InfoAgent] Respuesta insuficiente en MCP. "
                            "Sugerir confirmaci√≥n con el encargado."
                        ),
                    )
                return "ESCALATION_REQUIRED"

            log.info(f"‚úÖ [InfoAgent] Respuesta factual: {respuesta_final[:200]}")
            return respuesta_final or "ESCALATION_REQUIRED"

        except Exception as e:
            log.error(f"üí• Error en InfoAgent.invoke: {e}", exc_info=True)
            if self.memory_manager and chat_id:
                self.memory_manager.update_memory(
                    chat_id, role="system",
                    content=f"[InfoAgent] Error interno: {e}"
                )
            return "ESCALATION_REQUIRED"
