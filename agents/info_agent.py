"""
üìö InfoAgent v3 (modo factual y sin invenciones)
==========================================================================================
Responde preguntas generales sobre el hotel: servicios, horarios, pol√≠ticas, etc.
Usa exclusivamente la base de conocimientos (API HTTP del MCP Server)
y escala al encargado si no hay informaci√≥n v√°lida.
"""

import re
import logging
import asyncio
from langchain_openai import ChatOpenAI

from core.language_manager import language_manager
from core.utils.normalize_reply import normalize_reply
from core.mcp_client import call_knowledge_base  # üëà usamos el nuevo m√©todo HTTP
from core.utils.time_context import get_time_context
from agents.interno_agent import InternoAgent  # üëà Escalaci√≥n interna

log = logging.getLogger("InfoAgent")

ESCALATE_SENTENCE = (
    "üïì Un momento por favor, voy a consultarlo con el encargado. "
    "Perm√≠teme contactar con el encargado."
)

# =====================================================
# üîç Helper: detectar si parece volcado t√©cnico interno
# =====================================================
def _looks_like_internal_dump(text: str) -> bool:
    """
    Detecta si el texto parece un volcado t√©cnico o contenido interno,
    pero permite Markdown normal de la base de conocimientos.
    Se ha ajustado para no escalar cuando el texto contiene t√©rminos reales del hotel.
    """
    if not text:
        return False

    dump_patterns = [
        "traceback", "error", "exception",
        "{", "}", "SELECT ", "INSERT ", "sql", "schema"
    ]
    if any(pat.lower() in text.lower() for pat in dump_patterns):
        return True

    keywords_ok = [
        "gimnasio", "desayuno", "recepci√≥n", "parking",
        "mascotas", "wifi", "check-in", "restaurante",
        "habitaciones", "coworking", "lavander√≠a"
    ]
    if len(text.split()) > 1200 and not any(k in text.lower() for k in keywords_ok):
        return True

    return False


# =====================================================
# üß© Tool principal (consulta HTTP factual)
# =====================================================
async def hotel_information_tool(query: str) -> str:
    """
    Devuelve respuesta directamente desde la base de conocimientos (API HTTP del MCP Server),
    sin generaci√≥n adicional ni resumen.
    """
    try:
        q = (query or "").strip()
        if not q:
            return ESCALATE_SENTENCE

        # üëá Nueva llamada directa al endpoint HTTP del servidor MCP
        result = await call_knowledge_base(q)

        if not result or "error" in result:
            log.error(f"‚ùå Error o respuesta nula desde knowledge_base: {result}")
            return ESCALATE_SENTENCE

        if not result.get("data"):
            log.warning("‚ö†Ô∏è La base de conocimientos no devolvi√≥ resultados.")
            return ESCALATE_SENTENCE

        # ‚úÖ Tomamos el contenido textual de los documentos
        docs = result.get("data", [])
        cleaned_text = "\n".join(d.get("content", "") for d in docs if isinstance(d, dict))

        if not cleaned_text.strip():
            log.warning("‚ö†Ô∏è Respuesta vac√≠a o sin texto v√°lido.")
            return ESCALATE_SENTENCE

        cleaned = normalize_reply(cleaned_text, q, "InfoAgent").strip()

        if not cleaned or len(cleaned) < 10:
            log.warning("‚ö†Ô∏è Respuesta demasiado corta en KB.")
            return ESCALATE_SENTENCE
        if _looks_like_internal_dump(cleaned):
            log.warning("‚ö†Ô∏è Dump t√©cnico detectado, escalando.")
            return ESCALATE_SENTENCE
        if "no hay resultados" in cleaned.lower() or "no encontrado" in cleaned.lower():
            return ESCALATE_SENTENCE

        cleaned = re.sub(r"[*#>\-]+", "", cleaned)
        cleaned = re.sub(r"\s{2,}", " ", cleaned).strip()

        log.info(f"‚úÖ [InfoAgent] Respuesta factual KB: {cleaned[:200]}")
        return cleaned

    except Exception as e:
        log.error(f"‚ùå Error en hotel_information_tool: {e}", exc_info=True)
        return ESCALATE_SENTENCE


# =====================================================
# üè® Clase InfoAgent (con memoria integrada, sin AgentExecutor)
# =====================================================
class InfoAgent:
    """
    Subagente que responde preguntas generales sobre el hotel.
    Escala autom√°ticamente al encargado si no hay informaci√≥n √∫til.
    Ahora integra memoria persistente por chat_id.
    """

    def __init__(self, model_name: str = "gpt-4.1-mini", memory_manager=None):
        self.model_name = model_name
        self.llm = ChatOpenAI(model=self.model_name, temperature=0.2)
        self.interno_agent = InternoAgent(memory_manager=memory_manager)
        self.memory_manager = memory_manager
        log.info("‚úÖ InfoAgent inicializado (modo factual).")

    # --------------------------------------------------
    def _sync_run(self, coro, *args, **kwargs):
        """Ejecuta funciones async en contexto sync."""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()
        return loop.run_until_complete(coro(*args, **kwargs))

    # --------------------------------------------------
    async def invoke(self, user_input: str, chat_history: list = None, chat_id: str = None) -> str:
        """
        Entrada principal del subagente (modo factual).
        Si no hay informaci√≥n en la KB ‚Üí escalaci√≥n autom√°tica.
        """
        log.info(f"üì© [InfoAgent] Consulta: {user_input}")
        lang = language_manager.detect_language(user_input)
        chat_history = chat_history or []

        try:
            respuesta_final = await hotel_information_tool(user_input)
            respuesta_final = language_manager.ensure_language(respuesta_final, lang)

            # üíæ Guardar en memoria (consulta y respuesta)
            if self.memory_manager and chat_id:
                self.memory_manager.update_memory(
                    chat_id,
                    role="assistant",
                    content=f"[InfoAgent] Entrada: {user_input}\n\nRespuesta factual: {respuesta_final}"
                )

            # üö® Detecci√≥n de falta de informaci√≥n
            no_info = any(
                p in respuesta_final.lower()
                for p in [
                    "no dispongo", "no tengo informaci√≥n", "consultarlo con el encargado",
                    "perm√≠teme contactar", "no hay resultados", "no encontrado"
                ]
            )

            if _looks_like_internal_dump(respuesta_final) or no_info or respuesta_final == ESCALATE_SENTENCE:
                log.warning("‚ö†Ô∏è Escalaci√≥n autom√°tica: no se encontr√≥ informaci√≥n √∫til.")
                msg = (
                    f"‚ùì *Consulta del hu√©sped:*\n{user_input}\n\n"
                    "üß† *Contexto:*\nEl sistema no encontr√≥ informaci√≥n relevante en la base de conocimiento."
                )

                # üß† Registrar escalaci√≥n tambi√©n en memoria
                if self.memory_manager and chat_id:
                    self.memory_manager.update_memory(
                        chat_id,
                        role="system",
                        content="[InfoAgent] Escalaci√≥n autom√°tica al encargado por falta de informaci√≥n factual."
                    )

                await self.interno_agent.escalate(
                    guest_chat_id=chat_id,
                    guest_message=user_input,
                    escalation_type="info_no_encontrada",
                    reason="Falta de informaci√≥n relevante en la base de conocimiento.",
                    context="Escalaci√≥n autom√°tica desde InfoAgent (modo factual)"
                )
                return language_manager.ensure_language(ESCALATE_SENTENCE, lang)

            log.info(f"‚úÖ [InfoAgent] Respuesta final factual: {respuesta_final[:200]}")
            return respuesta_final or ESCALATE_SENTENCE

        except Exception as e:
            log.error(f"üí• Error en InfoAgent.invoke: {e}", exc_info=True)

            if self.memory_manager and chat_id:
                self.memory_manager.update_memory(
                    chat_id,
                    role="system",
                    content=f"[InfoAgent] Error interno en procesamiento: {e}"
                )

            await self.interno_agent.escalate(
                guest_chat_id=chat_id,
                guest_message=user_input,
                escalation_type="error_runtime",
                reason="Error en ejecuci√≥n del InfoAgent",
                context="Error interno durante la invocaci√≥n"
            )

            return language_manager.ensure_language(ESCALATE_SENTENCE, lang)
