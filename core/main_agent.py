import os
import logging
from langchain_openai import ChatOpenAI
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate
from tools.hotel_tools import get_all_hotel_tools
from core.language import detect_language, enforce_language
from core.db import save_message, get_conversation_history
from core.utils.utils_prompt import load_prompt


# =====================================================
# üè® Agente h√≠brido principal del sistema HotelAI
# =====================================================
class HotelAIHybrid:
    """
    Sistema h√≠brido de IA para HotelAI.
    Usa LangChain Functions + prompts externos + herramientas espec√≠ficas.
    """

    def __init__(self):
        # ‚öôÔ∏è Configuraci√≥n del modelo desde entorno (.env)
        self.model_name = os.getenv("OPENAI_MODEL", "gpt-5-mini")
        self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
        logging.info(f"üß† Inicializando HotelAIHybrid con modelo: {self.model_name}")

        # üß© Inicializar modelo LLM
        self.llm = self._build_llm()

        # üß∞ Cargar herramientas y crear el agente
        self.tools = get_all_hotel_tools()
        self.agent = self._create_agent()

        # ‚öôÔ∏è Crear executor LangChain
        self.executor = AgentExecutor.from_agent_and_tools(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
            return_intermediate_steps=False,
        )

        logging.info("‚úÖ HotelAIHybrid inicializado correctamente.")

    # -------------------------------------------------
    # üß† Inicializaci√≥n del modelo con fallback
    # -------------------------------------------------
    def _build_llm(self):
        """Intenta usar el modelo especificado; si falla, usa gpt-4o-mini."""
        try:
            llm = ChatOpenAI(
                model=self.model_name,
                temperature=self.temperature,
                max_tokens=1500,
                streaming=False,
            )
            # Prueba m√≠nima de conectividad
            _ = llm.invoke("ping")
            logging.info(f"‚úÖ Modelo {self.model_name} cargado correctamente.")
            return llm
        except Exception as e:
            logging.warning(
                f"‚ö†Ô∏è No se pudo cargar {self.model_name}: {e}. "
                "Usando 'gpt-4o-mini' como respaldo."
            )
            return ChatOpenAI(
                model="gpt-4o-mini",
                temperature=self.temperature,
                max_tokens=1500,
                streaming=False,
            )

    # -------------------------------------------------
    # üß© Creaci√≥n del agente con prompt externo
    # -------------------------------------------------
    def _create_agent(self):
        """Crea el agente LangChain con tools + prompt del archivo /prompts."""
        system_prompt = load_prompt("main_prompt.txt")
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("user", "{input}"),
            ("assistant", "{agent_scratchpad}")
        ])
        return create_openai_functions_agent(self.llm, self.tools, prompt)

    # -------------------------------------------------
    # üí¨ Procesamiento principal del mensaje
    # -------------------------------------------------
    async def process_message(self, user_message: str, conversation_id: str = None):
        """
        Procesa un mensaje del usuario usando el agente h√≠brido.
        conversation_id debe ser el n√∫mero del usuario (sin '+').
        """
        # ‚úÖ Usar el n√∫mero del usuario como ID de conversaci√≥n
        if not conversation_id:
            logging.warning("‚ö†Ô∏è conversation_id no recibido ‚Äî usando ID temporal.")
            conversation_id = "unknown"

        clean_id = str(conversation_id).replace("+", "").strip()
        language = detect_language(user_message)
        logging.info(f"üì© Mensaje recibido de {clean_id} ({language}): {user_message}")

        try:
            # üîÅ Recuperar historial previo para contexto (√∫ltimos 5 mensajes)
            history = get_conversation_history(clean_id, limit=5)
            history_context = "\n".join(
                [f"{m['role']}: {m['content']}" for m in history]
            )

            full_input = (
                f"Historial reciente:\n{history_context}\n\n"
                f"Nuevo mensaje del usuario:\n{user_message}"
            )

            # ü§ñ Ejecutar agente
            result = await self.executor.ainvoke({
                "input": full_input,
                "language": language,
            })
            output = result.get("output", "").strip()
            logging.info(f"ü§ñ Respuesta generada: {output[:150]}...")

        except Exception as e:
            logging.error(f"‚ùå Error en agente: {e}", exc_info=True)
            output = (
                "Ha ocurrido un error procesando tu solicitud. "
                "Estoy contactando con el encargado."
            )

        # üó£Ô∏è Ajustar idioma de salida
        final_response = enforce_language(user_message, output, language)

        # üíæ Guardar conversaci√≥n en Supabase
        save_message(clean_id, "user", user_message)
        save_message(clean_id, "assistant", final_response)
        logging.info(f"üíæ Conversaci√≥n {clean_id} guardada correctamente.")

        return final_response
