import json
import re
import logging
from langchain_openai import ChatOpenAI


# -----------------------------------------------------
# üîπ Limpieza robusta de respuesta cruda (sin LLM)
# -----------------------------------------------------
def normalize_reply(raw_reply, query=None, source=None):
    """
    Normaliza respuestas crudas que vienen desde MCP o agentes secundarios.
    - Soporta dicts, JSON, listas o strings planos.
    - Evita devolver valores vac√≠os que activen el fallback.
    - Conserva el texto incluso si el formato no es est√°ndar.
    """
    try:
        # üî∏ Caso nulo
        if raw_reply is None:
            return ""

        # üî∏ Si viene como dict (p. ej. {"text": "..."} o {"pageContent": "..."})
        if isinstance(raw_reply, dict):
            for key in ["pageContent", "text", "content", "response"]:
                if key in raw_reply and isinstance(raw_reply[key], str):
                    val = raw_reply[key].strip()
                    if val:
                        return val
            # Devuelve el JSON como texto si no hay campos reconocibles
            return json.dumps(raw_reply, ensure_ascii=False)

        # üî∏ Si es JSON string
        if isinstance(raw_reply, str):
            try:
                obj = json.loads(raw_reply)
                if isinstance(obj, dict):
                    for key in ["pageContent", "text", "content", "response"]:
                        if key in obj and isinstance(obj[key], str):
                            val = obj[key].strip()
                            if val:
                                return val
                    # Si no hay campos reconocibles, devuelve JSON completo
                    return json.dumps(obj, ensure_ascii=False)
            except Exception:
                pass  # no era JSON v√°lido, se trata como texto normal

        # üî∏ Si es lista (varios resultados o fragmentos)
        if isinstance(raw_reply, list):
            parts = []
            for item in raw_reply:
                if isinstance(item, dict):
                    for key in ["pageContent", "text", "content"]:
                        if key in item and item[key]:
                            parts.append(str(item[key]).strip())
                            break
                elif isinstance(item, str):
                    parts.append(item.strip())
            if parts:
                return "\n".join(parts).strip()

        # üî∏ Si es texto plano
        if isinstance(raw_reply, str):
            cleaned = re.sub(r"\s+", " ", raw_reply).strip()
            return cleaned or raw_reply

        # üî∏ Fallback final: convierte cualquier cosa a texto
        return str(raw_reply)

    except Exception as e:
        logging.error(f"‚ö†Ô∏è Error en normalize_reply: {e}", exc_info=True)
        return str(raw_reply) or "Respuesta no disponible"


# -----------------------------------------------------
# üí¨ Post-procesamiento con LLM (estilo n8n)
# -----------------------------------------------------
def summarize_tool_output(query: str, raw_output: str, temperature: float = 0.0) -> str:
    """
    Reformula la salida cruda de una tool en una respuesta natural y amable.
    Equivale al "LLM Post-Processor" de n8n.
    """
    try:
        if not raw_output or len(raw_output.strip()) == 0:
            return "Lo siento, no encontr√© informaci√≥n relevante en este momento."

        llm = ChatOpenAI(model="gpt-4o-mini", temperature=temperature)

        prompt = f"""
            Eres el asistente virtual del hotel. Responde de forma amable,
            natural y √∫til al hu√©sped, usando la informaci√≥n a continuaci√≥n.

            Consulta del hu√©sped:
            "{query}"

            Informaci√≥n encontrada:
            {raw_output}

            Instrucciones:
            - Responde en el mismo idioma que la consulta.
            - Da una respuesta breve (2‚Äì4 frases), clara y natural.
            - No uses formato JSON ni listas t√©cnicas.
            - Si la informaci√≥n indica ‚Äúno disponible‚Äù, responde con una frase educada explicando la situaci√≥n.
            """

        response = llm.invoke(prompt)
        return response.content.strip()

    except Exception as e:
        logging.error(f"‚ö†Ô∏è Error en summarize_tool_output: {e}", exc_info=True)
        # fallback simple: devuelve la respuesta sin reformular
        return raw_output.strip()
